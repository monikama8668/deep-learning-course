{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. The challenge\n",
    "2. Data loading\n",
    "3. Data preprocessing\n",
    "4. Compile the CNN\n",
    "5. Make new predictions\n",
    "6. Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-datasets-images/144468/337794/1ad8516fa7ee2c9eadfbdda2b62a7b20/data-original.jpg?t=2019-03-21-04-51-44\"/>\n",
    "\n",
    "Given an image data-set of cats and dogs, train a CNN to differentiate between them and test it out on new images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the data-set for this exercise go to [https://github.com/georgiosouzounis/dogs_and_cats](https://github.com/georgiosouzounis/dogs_and_cats) and follow the instructions.\n",
    "\n",
    "To obtain the original data-set from [Kaggle.com](kaggle.com), login in to your account and download it from: [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two datasets, the training and test set. Images in each set differ between them in size, dimensions, color range and semantics (i.e. background scenes)\n",
    "\n",
    "\n",
    "| training data-set   | test data-set       |\n",
    "|---------------------|---------------------|\n",
    "| 4000 images of dogs | 1000 images of dogs |\n",
    "| 4000 images of cats | 1000 images of cats |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://bit.ly/29ltoLY\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to:\n",
    "\n",
    "- Normalize the intensities of all images;\n",
    "- Normalize the size of all images and pad the gaps;\n",
    "- Create new images to capture unseen ‘views’;\n",
    "- Set the way images will be ‘fed’ into the network;\n",
    "- Specify the batch size.\n",
    "\n",
    "Keras provides a single tool for all that, the [ImageDataGenerator](https://keras.io/preprocessing/image/)  from the Image Preprocessor library. It is primarily used for generating batches of tensor image data with real-time data augmentation. The data will be looped over (in batches). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import the keras image data generator library\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image normalization and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project data (images) are separated in two directories, the training and testing images. \n",
    "\n",
    "All images need to be rescaled (intensity) to the range of 0 to 1. Given that each of the red, green and blue channels have a highest possible value of 255, re-scaling is done by dividing with 255 (first parameter of the ImageDataGenerator) \n",
    "\n",
    "Optionally you may choose to perform contrast correction operations beforehand, as shown on the left.\n",
    "\n",
    "Next set shear and zoom range values and enable the horizontal flip option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the training image generator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# configure the test image generator\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shear, zoom and horizontal flip generate image distortions that mimic possible object perception from non-available viewing angles.\n",
    "\n",
    "The image on the left shows an example of the STOP traffic sign distorted in a number of different ways.\n",
    "\n",
    "[Example of image transformations:](https://chatbotslife.com/german-sign-classification-using-deep-learning-neural-networks-98-8-solution-d05656bf51ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*_mgTLhXwWGDEgz_2C7dRDg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example of traffice-sign pre-processing](https://campushippo.com/lessons/build-a-tensorflow-traffic-sign-classifier-at-95-accuracy-214727f24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.filestackcontent.com/xQUoihTkRLA4CBtb3Cll\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the datasets \n",
    "\n",
    "Each image set (training and testing) can be now accessed using a pointer to the respective ImageDataGenerator object of Keras and using the [flow_from_directory()](https://keras.io/preprocessing/image/) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*IqWwrQJk2-iILjKCAdPQ6w.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Importing the training dataset\n",
    "training_set = train_datagen.flow_from_directory('dogs_and_cats/train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Importing the test dataset\n",
    "test_set = test_datagen.flow_from_directory('dogs_and_cats/test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we need to specify:\n",
    "\n",
    "- the target image patch size to which each image will be resized to;\n",
    "- the batch size, i.e. how many images will be thrown to the CNN at each time;\n",
    "- and the class annotation mode; binary in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us load the necessary libraries from the Keras API:\n",
    "\n",
    "- Import the [sequential model](https://keras.io/getting-started/sequential-model-guide/);\n",
    "- Import the [2D convolution layer](https://keras.io/layers/convolutional/);\n",
    "- Import the [2D max-pooling layer](https://keras.io/layers/pooling/);\n",
    "- Import the [flatten layer](https://keras.io/layers/core/);  \n",
    "- Import the [Dense layer](https://keras.io/layers/core/);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN initialization\n",
    "\n",
    "Create an instance of the sequential model called classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.samyzaf.com/ML/pima/nn6.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Convolution Layer\n",
    "\n",
    "Add a convolutional layer using the add method of the sequential model. \n",
    "\n",
    "The first argument in the layer type.\n",
    "\n",
    "We can now customize the:\n",
    "\n",
    "- Number of filters (neurons) = 32,\n",
    "- Each Filter size (3x3),\n",
    "- The image input shape: 64 x 64 pixels x 3 channels (RGB);\n",
    "- The layer activation function function: ReLU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Pooling Layer\n",
    "\n",
    "Lets us add a pooling layer to subsample the input image and capture its properties at half the scale.\n",
    "\n",
    "We use the same macro as before, **classifier.add()**, specify the layer type to be **MaxPooling2D**, and customize the pooling parameter to **(2,2)**, i.e divide each input dimension by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Second Convolution Layer\n",
    "\n",
    "We add a second convolution layer to compute the same filters on the subsampled feature map, i.e. the output of the last pooling layer.\n",
    "\n",
    "When done we apply a max-pooling once again to reduce the output size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "\n",
    "We compute a layer flattening on the output of the second max-pooling layer. \n",
    "\n",
    "This generates stand alone units that can be directed in a conventional ANN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening: convert pixels to input nodes\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection Layers\n",
    "\n",
    "The ANN complementing the CNN has 1 hidden layer and 1 output. Both are designated as ‘dense layers’. \n",
    "\n",
    "The **hidden layer** can be configured with any number of neurons, **128** in this case but that can grow if dropout is to be introduced. It has a **ReLU** activation layer.\n",
    "\n",
    "The **output layer** has one unit since we want a single label out. If activated the image shows a dog and if not the image shows a cat. To convert the output probability to a class label we use the **sigmoid activation function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression (a.k.a. MaxEnt, multinomial logistic regression, softmax Regression, Maximum Entropy Classifier). [Read more on their differences](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the CNN\n",
    "\n",
    "Finally, lets compile the classifier configured with:\n",
    "\n",
    "- the [Adam optimizer](https://keras.io/optimizers/) for utilizing the **Stochastic Gradient Descent** approach: [article](https://arxiv.org/abs/1412.6980v8) \n",
    "- the [binary_crossentropy](https://keras.io/losses/) as a loss function (or objective function, or optimization score function) which is optimal for binary classification problems;\n",
    "- the [accuracy metric](https://keras.io/metrics/). A metric is a function that is used to judge the performance of your model. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', \n",
    "                         loss = 'binary_crossentropy', \n",
    "                         metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the CNN to the Training set\n",
    "\n",
    "By contrast to the fit() function used in ANNs, we will know use the fit_generator(). For a detailed discussion on the differences between the two read:\n",
    "\n",
    "[A thing you should know about Keras if you plan to train a deep learning model on a large dataset](https://towardsdatascience.com/keras-a-thing-you-should-know-about-keras-if-you-plan-to-train-a-deep-learning-model-on-a-large-fdd63ce66bd2).\n",
    "\n",
    "The arguments in order are:\n",
    "\n",
    "- the training set configured earlier;\n",
    "- the number of steps in each epoch;\n",
    "- the number of epochs (re-runs);\n",
    "- the validation data (validation is an internal process);\n",
    "- the number of validation steps;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*_lQtIO-FvsPcyq3n2HyyCg.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 1442s 180ms/step - loss: 0.3431 - acc: 0.8459 - val_loss: 0.5553 - val_acc: 0.7935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f337ef46eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit_generator(training_set,\n",
    "                                 steps_per_epoch = 8000,\n",
    "                                 epochs = 25,\n",
    "                                 validation_data = test_set,\n",
    "                                 validation_steps = 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the CNN to the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the earlier slide on importing the datasets, we used the flow_from_directoty() function differs from flow as shown on the left diagram.\n",
    "\n",
    "[steps_per_epoch](https://keras.io/models/sequential/): Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples of your dataset divided by the batch size.\n",
    "\n",
    "In this exercise we set it to be equal to the number of unique samples but this comes at an increase in computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*IqWwrQJk2-iILjKCAdPQ6w.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Ndew Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it a cat or a dog?\n",
    "\n",
    "A new observation (image) is given. Given the model we trained can we classify the new image as one showing a cat or one showing a dog.\n",
    "\n",
    "<img src=\"https://cdn.editorchoice.com/wp-content/uploads/2019/06/dogtilt.jpg\" width=\"300\"/> <img src=\"https://boygeniusreport.files.wordpress.com/2017/01/cat.jpg?quality=98&strip=all\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "We need \n",
    "- **NumPy** to store new images for compatibility issues;\n",
    "- the **image** module of the Keras preprocessing library. It will be used for loading new images and for converting them to NumPy arrays as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting a single new observation\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess New Data\n",
    "\n",
    "We can now load the new image using the image module and reshape it to the target dimensions.\n",
    "\n",
    "The image is then converted to a NumPy array and a new dimension is added as required by the Keras classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new image\n",
    "test_image = image.load_img('dogs_and_cats/exercise/what_is_it1.jpg', target_size = (64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to an array\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra dimension for compatibility\n",
    "test_image = np.expand_dims(test_image, axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the classifier on the new image\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the class indices from the training set\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the classifier result is 1 the prediction is a dog, or a # cat otherwise\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Love the challenges\n",
    "\n",
    "|example | example |\n",
    "|---|---|\n",
    "|<img src=\"https://pm1.narvii.com/6568/43456984beeff7563e2865947ff646cc2b200b77_hq.jpg\" width=\"300\"/> | <img src=\"https://fc03.deviantart.net/fs22/f/2007/347/a/e/Catdog_by_Sannebe.jpg\" width=\"300\"/>|\n",
    "|<img src=\"http://photoshopcontest.com/images/fullsize/sgsaavsxlq4bvgkjcpaaa9jsykqkih08ang1.jpg\" width=\"300\"/> | <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCTzNzCPox2GdcpQCcsvZ3xFTWYX05DVmPrkxUZEDmdPyuoWPl\" width=\"300\"/>|\n",
    "|<img src=\"https://i.ytimg.com/vi/bxKoHe7m59E/maxresdefault.jpg\" width=\"300\"/> | <img src=\"https://i.chzbgr.com/full/9091640832/h68B68331/\" width=\"300\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "To understand the performance of your model’s architecture it is sufficient to run it over the validation set. Subsequently k-Fold Cross Validation is not needed.\n",
    "\n",
    "Classical approaches for model improvement include adjusting the:\n",
    "- optimizer,\n",
    "- number of epochs,\n",
    "- loss function,\n",
    "- dropout,\n",
    "- metrics, etc.\n",
    "\n",
    "You may wish however to adjust the network architecture by including more convolution and dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
