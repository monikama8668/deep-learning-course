{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "Prof. Dr. Georgios K. Ouzounis\n",
    "<br/>email: [georgios.ouzounis@go.kauko.lt](georgios.ouzounis@go.kauko.lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Challenge\n",
    "2. Getting the data-set\n",
    "3. Data loading and pre-processing\n",
    "4. Compiling the ANN\n",
    "5. Deploying the ANN\n",
    "6. Testing individual cases\n",
    "7. Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "We are given a data set consisting of 14 features and a total of 10000 records of customers of a financial institution. \n",
    "\n",
    "Among the features there is one tagged as **Exited** that takes binary values and if true it means that the given customer rejected a product or if false that he/she retained it.\n",
    "\n",
    "The goal of this exercise is to train a model that can predict as accurately as possible the decision of new customers and using the same features. \n",
    "\n",
    "\n",
    "<img src=\"https://www.cuinsight.com/wp-content/uploads/2015/07/27178-d21ed95f.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data-set\n",
    "\n",
    "The data-set is a comma-separated values file (CSV) and contains a sample data table of 10000 records. It can be found at the [Kaggle.com website](https://www.kaggle.com/aakash50897/churn-modellingcsv) or at various web locations after searching for its filename:\n",
    "\n",
    "**Churn_Modelling.csv**\n",
    "\n",
    "Example locations: \n",
    "\n",
    "- [Floobits.com](https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv)\n",
    "- [Pushkar Mandot (Medium blog writer) G-drive](https://drive.google.com/file/d/0By9Y49AzZGaUemtpNWtQMWdqRDA/view)\n",
    "\n",
    "Open a terminal session:\n",
    "\n",
    "<img src=\"https://www.macworld.co.uk/cmsdata/features/3608274/Terminalicon2_thumb800.png\" align=\"left\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the wget command to put it of the selected location\n",
    "\n",
    "```shell\n",
    "wget https://floobits.com/calvinlow18/ANN/raw/Churn_Modelling.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and pre-processing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. \n",
    "\n",
    "[matplotlib](https://matplotlib.org):  it is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "\n",
    "[pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import & explore the dataset\n",
    "\n",
    "The variable dataset is a python dataframe holding the contents of the opened file. To scout it’s contents use the **info()** and **head()** functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the features\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the head of the file\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning/ Splitting\n",
    "\n",
    "The **independent variables** are to be stored in matrix X. Evidently, neither to row ID (column 0) nor the customer number (column 1) nor the surname (column 2) can influence the decision of the customer thus we can read the all other features leaving these two out.\n",
    "\n",
    "The **dependent variable**, i.e. the one we want to predict, is to be stored on a separate matrix (vector) y and contains the contents of column 13 alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the independent variables stored in columns 3 to 12 \n",
    "# are stored in X \n",
    "X = dataset.iloc[:, 3:13].values \n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column index 13 : the dependent variables\n",
    "y = dataset.iloc[:, 13].values \n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "\n",
    "The independent variables **Geography** and **Gender** are **strings**, or **objects** using **info()** function, that need to be encoded into discrete variables as discussed previously \n",
    "in the **Features** session.\n",
    "\n",
    "**LabelEncoder** takes in as argument the column index and converts all categorical entries to integer labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geography column: enumerate countries\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender column: enumerate female/male\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding introduces a new problem in our data. LabelEncoder has replaced France with 0, Germany 1 and Spain 2 but Germany is not higher than France and France is not smaller than Spain so we need to create a dummy variable for Country. \n",
    "\n",
    "Read more on [dummy variables here](https://en.wikiversity.org/wiki/Dummy_variable_%28statistics%29).\n",
    "\n",
    "We don’t need to do same for **Gender** variable as it is binary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy variables using the ScikitLearn library function called [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). It takes the column number as input and returns the minimum number of columns needed to encode the categories with unique combinations of 0s & 1s. [Read more here](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f).\n",
    "\n",
    "Read more on the [dummy variable trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html) :: remove redundancies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode enumerations to switch tables to prevent conflicts\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first column to avoid the dummy variable trap\n",
    "X = X[:, 1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset to training and testing sets\n",
    "\n",
    "Next, we need to divide our data set to two subsets, one for testing and one for training. \n",
    "ScikitLearn library provides the function [train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):\n",
    "\n",
    "**sklearn.model_selection.train_test_split()**\n",
    "\n",
    "that splits arrays or matrices into random train and test subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Feature scaling is essential as discussed if the **Features** lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "That is simply because some variables have values in the thousands while some others have values is the tens or ones. It is very important to ensure that none of our variables  dominate over the others.\n",
    "\n",
    "It is computed using the ScikitLearn library [StandardScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which is fitted in the training set and applied to both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the keras libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" width=\"400\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the sequential model from the Keras API to initialize our ANN;\n",
    "- Import the Dense layer template from the Keras API to add hidden layers;\n",
    "- Create an instance of the sequential model called classifier since our job is in the classification domain.\n",
    "\n",
    "The Dense layer is a layer in which all inputs are connected to all outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Dense layer added to our classifier:\n",
    "\n",
    "- consists of 6 units (neurons), thus generating 6 outputs;\n",
    "- has a uniform kernel initialization (weight matrix);\n",
    "- applies a ReLU activation function on the output of each unit;\n",
    "- takes a 11 inputs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Second Hidden Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second Dense layer added to our classifier:\n",
    "\n",
    "- consists of 6 units (neurons), thus generating 6 outputs;\n",
    "- has a uniform kernel initialization (weight matrix);\n",
    "- applies a ReLU activation function on the output of each unit;\n",
    "- takes as input the outputs of the previous layer; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output Dense layer added to our classifier:\n",
    "\n",
    "- consists of 1 unit (neuron), thus generating a binary output;\n",
    "- has a uniform kernel initialization (weight matrix);\n",
    "- applies a Sigmoid activation function on the output of the single unit;\n",
    "- takes as input the outputs of the previous layer; \n",
    "\n",
    "If the number of categories in the output layer is more than 2 we then need to use the SoftMax activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model compilation we customize the:\n",
    "    \n",
    "- [Optimizer](https://keras.io/optimizers/): is the algorithm used to find optimal set of weights. Adam employs Stochastic Gradient Descent (SGD)!\n",
    "- [Loss function](https://keras.io/losses/#available-loss-functions): SGD requires a loss function. With binary outputs we use a logarithmic loss function called the binary_crossentropy. If the dependent variable was categorical, i.e. taking more than 2 values, we would have used the categorical_crossentropy.\n",
    "- [Metric](https://keras.io/metrics/): this is the metric used for model improvement; we use accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the ANN to the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our ANN using the data in our training set X and our class labels (dependent variables) in y. Parameters that can be specified are the:\n",
    "\n",
    "- Batch size: specifies the number of observations fed into the model after which the weight matrix is updated. \n",
    "- Number of epochs: number of iterations of the whole process!\n",
    "\n",
    "[more here](https://keras.io/models/model/#fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: using the trained ANN on our Training set X, lets see how well it performs on our Test set for which we have ground truth, i.e. we know the results.\n",
    "\n",
    "For each probability returned we generate a categorical outcome (true/false) by thresholding it at a value of 50% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold the probabilities into True > 0.5 or False\n",
    "y_pred = (y_pred > 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. Use the ScikitLearn library [confucion_matrix()](https://en.wikipedia.org/wiki/Confusion_matrix) function to compute it and display it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sum(cm)\n",
    "s2 = sum(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (cm[0,0] + cm[1,1])/s2\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing individual cases\n",
    "\n",
    "In this lecture we will learn how to predict the behaviour of an new data sample outside our training and test data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new observation (data entry) is given. Given the model we trained can we predict if this new customer is likely to stay or to go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://catalystforbusiness.com/wp-content/uploads/2017/12/customer-care.jpg\" align=\"left\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New customer data\n",
    "\n",
    "| Geography | Credit Score | Gender | Age | Tenure | Balance | Number of Products | Has Credit Card | Is Active Member | Estimated Salary | \n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| France | 600 | Male | 40 | 3 | 60000 | 2 | Yes | Yes | 50000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format as in the case of the training/test sets.\n",
    "\n",
    "1. Create a new NP array and populate it accordingly.\n",
    "2. Use sc.transform to transform the vector to the desired format.\n",
    "3. Request a prediction and threshold it as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request a prediction from the ANN using the new data formatted as needed;\n",
    "# set the first entry of the numpy array as float to avoid complaints\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))) # 2D array use row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction = (new_prediction > 0.5)\n",
    "new_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "In this lecture we will learn how to evaluate, improve and tune the ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Sequential Keras models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at keras.wrappers.scikit_learn.py.\n",
    "\n",
    "There are [two wrappers available](https://keras.io/scikit-learn-api/). Consider the first: keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, \\**sk_params), which implements the Scikit-Learn classifier interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the ANN\n",
    "\n",
    "# load the libraries\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Keras scikit_learn wrapper to compute some statistics about our ANN\n",
    "\n",
    "1. Create the equivalent sckit_learn compatible classifier.\n",
    "2. Parameterize it as before and run k-fold cross validation\n",
    "3. Obtain the metrics\n",
    "\n",
    "Define a function to configure your classifier as requested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our classifier\n",
    "\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile a Keras classifier for the sckit_learn library to compute the k-fold cross validation. The latter will produce a set of accuracy metrics for each run from which we aim at the mean\n",
    "\n",
    "Use these settings to Regularize the Dropout to reduce overfitting if necessary.\n",
    "\n",
    "- [Dropout Regularization in Deep Learning Models With Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n",
    "- [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n",
    "Browser compatibility for parallel execution using Jupyter notebooks: \n",
    "- test on Safari **Failed**, \n",
    "- test on Chrome **Pass** (~16min).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-fold cross validation\n",
    "\n",
    "# configure the classifier as needed; set the building function, the batch size and the number of epochs, as before\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the k-fold cross validation; n_jobs = number of cpus, when set to -1 it means use all\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = accuracies.std()\n",
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an insignificant change on our mean accuracy! This means that **no overfitting** occurs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If overfitting was to be observed, one way to counter it and make the model more general is by using dropout regularization. \n",
    "\n",
    "Dropout constraints the number of neurons that get activated in an arbitrary manner. The parameter p specifies (%wise) how many neurons to be switched off in each layer.\n",
    "\n",
    "We do not need to run this since no overfitting is observed in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this library\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11)) \n",
    "classifier.add(Dropout(p = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(p = 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Keras scikit_learn wrapper to compute some statistics about our ANN:\n",
    "\n",
    "1. Create the equivalent sckit_learn compatible classifier.\n",
    "2. Parameterize it as before, add more options and run k-fold cross validation for each parameter set\n",
    "3. Obtain global metrics and get the best settings/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries; note the Grid Search Cross Validation lib\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our classifier\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Keras classifier with no parameters. \n",
    "\n",
    "Create a separate vector of parameters, each with a number of different settings.\n",
    "\n",
    "Run GridSearchCV using the classifier as estimator, the parameters vector, and by specifying the number of k-folds and the scoring metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classifier as needed; set the building function\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter different options for the batch size, the number of epochs and the optimizer:\n",
    "parameters = {'batch_size': [25, 32], 'epochs': [100, 500], 'optimizer': ['adam', 'rmsprop']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the Grid Search CV\n",
    "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know which parameter setting from them all scores the highest accuracy.\n",
    "\n",
    "Printing out the best parameters we observe the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the grid_search model to our training data\n",
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the best parameters and best accuracy\n",
    "best_parameters = grid_search.best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_\n",
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1ssIjY7LC98PSTGfU9RlWpig-5pEjpD-r\" align=\"left\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
